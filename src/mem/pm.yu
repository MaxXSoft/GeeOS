import arch.arch
import arch.riscv.consts
import arch.riscv.addr
import sync.spinlock
import mem.consts
import lib.except
import lib.c.string

// structure of all free physical pages
struct FreePage {
  next: FreePage var*,
}

// physical memory structure
struct PhyMemory {
  lock: Spinlock,
  free_list: FreePage var*,
}
var phy_mem: PhyMemory


// free physical page
public def freePhyMem(addr: u8 var*) {
  // check if address is not aligned, or out of range
  if addr as usize % PAGE_SIZE as usize != 0 as usize ||
     addr < _geeos_end as u8 var* || addr >= HEAP_BASE as u8 var* {
    panic("freePhyMem")
  }
  // fill with junk to catch dangling refs
  memset(addr, 1, PAGE_SIZE as usize)
  // insert current page into 'free_list'
  let fp = addr as FreePage var*
  phy_mem.lock.acquire()
  (*fp).next = phy_mem.free_list
  phy_mem.free_list = fp
  phy_mem.lock.release()
}

// initialize physical page allocator
public def initPhyMem() {
  phy_mem.lock = newSpinlock()
  // free all avaliable physical pages
  let pa = newPhysAddr(_geeos_end as usize)
  let pa_4k = pa.roundUp()
  var p = pa_4k.getAddr() as u8 var*
  while p + PAGE_SIZE <= HEAP_BASE as u8 var* {
    freePhyMem(p)
    p += PAGE_SIZE
  }
}

// allocate physical page
public def allocPhyMem(): u8 var* {
  // pick a new free page from 'free_list'
  phy_mem.lock.acquire()
  let fp = phy_mem.free_list
  if fp != null as FreePage var* {
    phy_mem.free_list = (*fp).next
  }
  phy_mem.lock.release()
  // fill with junk
  if fp != null as FreePage var* {
    memset(fp as u8 var*, 5, PAGE_SIZE as usize)
  }
  fp as u8 var*
}
